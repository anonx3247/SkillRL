---
phase: 02-skill-system-evaluation
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/evaluation/__init__.py
  - src/evaluation/orchestrator.py
  - src/evaluation/metrics.py
  - src/evaluation/checkpoint.py
  - src/agent/loop.py
  - src/main.py
autonomous: true

must_haves:
  truths:
    - "Full 134-task evaluation runs with 10 concurrent workers using asyncio.Semaphore"
    - "Each task gets TopK=3 skills retrieved based on its description"
    - "Aggregate metrics computed: overall success rate, per-subtask success rate, avg steps for successes and failures"
    - "Iteration checkpoint saved atomically after evaluation completes, resumable via checkpoint_latest.json symlink"
    - "Iteration 0 baseline runs with empty skill library (no skills retrieved)"
    - "CLI supports `python -m src.main evaluate` to run full evaluation"
  artifacts:
    - path: "src/evaluation/orchestrator.py"
      provides: "Parallel task evaluation with semaphore-limited concurrency"
      contains: "class EvaluationOrchestrator"
    - path: "src/evaluation/metrics.py"
      provides: "Per-task and aggregate metrics computation"
      contains: "def compute_metrics"
    - path: "src/evaluation/checkpoint.py"
      provides: "Iteration checkpoint save/load with atomic writes"
      contains: "class CheckpointManager"
    - path: "src/agent/loop.py"
      provides: "run_task accepts optional retrieved_skills parameter"
      contains: "retrieved_skills"
    - path: "src/main.py"
      provides: "CLI with evaluate subcommand"
      contains: "evaluate"
  key_links:
    - from: "src/evaluation/orchestrator.py"
      to: "src/agent/loop.py"
      via: "calls run_task() with retrieved_skills"
      pattern: "run_task"
    - from: "src/evaluation/orchestrator.py"
      to: "src/skills/retrieval.py"
      via: "uses SkillRetriever.retrieve() per task"
      pattern: "retriever.retrieve"
    - from: "src/evaluation/orchestrator.py"
      to: "src/evaluation/metrics.py"
      via: "calls compute_metrics() on trajectories"
      pattern: "compute_metrics"
    - from: "src/evaluation/orchestrator.py"
      to: "src/evaluation/checkpoint.py"
      via: "saves checkpoint after evaluation"
      pattern: "checkpoint_manager.save"
    - from: "src/main.py"
      to: "src/evaluation/orchestrator.py"
      via: "CLI invokes orchestrator.run()"
      pattern: "orchestrator"
---

<objective>
Build the parallel evaluation orchestrator with metrics, checkpointing, and CLI integration. Wire skills into the agent loop. Run iteration 0 baseline (no skills).

Purpose: Enables full 134-task evaluation with concurrent execution, establishes the baseline performance metric (iteration 0, no skills), and provides stop/resume capability for long-running experiments.

Output: `src/evaluation/` package with orchestrator, metrics, checkpoint modules; updated `src/agent/loop.py` with skill support; updated `src/main.py` with evaluate CLI command.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-skill-system-evaluation/02-RESEARCH.md
@.planning/phases/02-skill-system-evaluation/02-01-SUMMARY.md
@src/agent/loop.py
@src/agent/prompts.py
@src/main.py
@src/environment/env_manager.py
@src/trajectory/models.py
@src/trajectory/storage.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire skills into agent loop and build evaluation orchestrator</name>
  <files>
    src/agent/loop.py
    src/evaluation/__init__.py
    src/evaluation/orchestrator.py
    src/evaluation/metrics.py
    src/evaluation/checkpoint.py
  </files>
  <action>
    1. **Update `src/agent/loop.py`** — Wire skill injection into `run_task()`. Three specific changes:

       **(a) Change import** (line ~10): Replace `from src.agent.prompts import AUTONOMOUS_AGENT_PROMPT` with `from src.agent.prompts import AUTONOMOUS_AGENT_PROMPT, build_prompt_with_skills`

       **(b) Add parameter to `run_task()` signature** (line ~326): Add `retrieved_skills: list | None = None` as the last parameter of `async def run_task(...)`.

       **(c) Replace hardcoded prompt in messages** (line ~357): Change `{"role": "system", "content": AUTONOMOUS_AGENT_PROMPT}` to `{"role": "system", "content": build_prompt_with_skills(retrieved_skills)}`. This way, when skills are passed they get injected; when None/empty, the base prompt is used unchanged.

       - No other changes to the agent loop logic.

    2. **Create `src/evaluation/__init__.py`** — Export EvaluationOrchestrator, compute_metrics, CheckpointManager.

    3. **Create `src/evaluation/metrics.py`** — Metrics computation:
       ```python
       from dataclasses import dataclass, asdict
       from collections import defaultdict
       from src.trajectory.models import Trajectory

       @dataclass
       class TaskMetrics:
           task_id: str
           task_type: str
           success: bool
           step_count: int
           skills_retrieved: int  # number of skills injected for this task

       @dataclass
       class AggregateMetrics:
           iteration: int
           overall_success_rate: float
           per_subtask_success_rate: dict[str, float]  # task_type -> rate
           avg_steps_success: float
           avg_steps_failure: float
           total_tasks: int
           successful_tasks: int

       def compute_metrics(trajectories: list[Trajectory], iteration: int, skills_per_task: dict[str, int] | None = None) -> AggregateMetrics:
       ```
       - Calculate overall_success_rate = successful / total
       - Group by task_type, compute per-subtask success rates
       - Compute avg_steps separately for successes and failures (0.0 if none)
       - Return AggregateMetrics dataclass

    4. **Create `src/evaluation/checkpoint.py`** — CheckpointManager:
       ```python
       @dataclass
       class IterationCheckpoint:
           iteration: int
           total_tasks: int
           successful_tasks: int
           overall_success_rate: float
           per_subtask_success_rate: dict[str, float]
           avg_steps_success: float
           avg_steps_failure: float
           skill_library_path: str  # path to skill library JSON at this iteration
           trajectories_path: str   # path to trajectories JSONL at this iteration
           timestamp: float

       class CheckpointManager:
           def __init__(self, checkpoint_dir: Path): ...
           def save(self, checkpoint: IterationCheckpoint): ...  # atomic write + update symlink
           def load_latest(self) -> IterationCheckpoint | None: ...  # read from symlink
       ```
       - Atomic write: temp + fsync + os.replace (same pattern as skills library and trajectory storage)
       - Symlink: `checkpoint_latest.json` -> `checkpoint_iter_N.json`
       - On macOS/Linux, use `os.replace` for symlink update (unlink + symlink_to)

    5. **Create `src/evaluation/orchestrator.py`** — EvaluationOrchestrator:
       ```python
       class EvaluationOrchestrator:
           def __init__(
               self,
               skill_library_path: Path,
               output_dir: Path,
               max_concurrent: int = 10,
               max_steps: int = 50,
               top_k_skills: int = 3,
           ): ...
       ```

       Key method: `async def run_iteration(self, iteration: int) -> AggregateMetrics`:

       **Step 1: Collect all 134 tasks** — Load a single EnvManager, call reset() 134 times sequentially to collect task descriptions, task IDs, and task types into a list of dicts. Store the task index with each entry. This is the "discovery pass" — the environment is sequential so we cannot parallelize this.

       **Step 2: Load skill library + retriever** — Load SkillLibrary from disk, create SkillRetriever, call `index_skills(library.get_all_skills())`. For iteration 0, library will be empty and retrieve() will return [].

       **Step 3: Run tasks in parallel** — Use `asyncio.Semaphore(max_concurrent)` to limit concurrency.

       For each task, inside the semaphore:
       - Create a fresh `EnvManager()`, call `load()`, then call `reset()` N+1 times to reach the correct task index
       - Retrieve TopK skills for this task's description
       - Call `run_task()` with `retrieved_skills=retrieved_skills`
       - Return the trajectory

       Use `tqdm.asyncio.tqdm_asyncio.gather()` for progress tracking.

       **CRITICAL: ALFWorld environment isolation.** Each concurrent worker MUST create its own EnvManager instance because ALFWorld uses global state. Do NOT share env_manager across workers.

       **Step 4: Compute metrics** — Call `compute_metrics(trajectories, iteration)`.

       **Step 5: Save results** — Save all trajectories via `append_trajectory()` to `output_dir/iter_{N}/trajectories.jsonl`. Copy skill library to `output_dir/iter_{N}/skills.json`. Save checkpoint via CheckpointManager.

       **Step 6: Print summary** — Display overall success rate, per-subtask rates, avg steps.

       Handle exceptions per-task gracefully: if a single task fails (API error, env error), log the error and create a failed Trajectory rather than crashing the entire evaluation. Use `try/except` inside the bounded task function.
  </action>
  <verify>
    Run: `python -c "
from src.evaluation import EvaluationOrchestrator, compute_metrics, CheckpointManager
from src.evaluation.metrics import AggregateMetrics, TaskMetrics
from src.evaluation.checkpoint import IterationCheckpoint
from src.trajectory.models import Trajectory, Step
import tempfile
from pathlib import Path

# Test metrics computation
trajs = [
    Trajectory(task_id='t1', task_description='d1', task_type='pick', success=True, steps=[], total_steps=5, duration_seconds=10.0, failure_reason=None, env_done=True),
    Trajectory(task_id='t2', task_description='d2', task_type='pick', success=False, steps=[], total_steps=50, duration_seconds=60.0, failure_reason='timeout', env_done=False),
    Trajectory(task_id='t3', task_description='d3', task_type='clean', success=True, steps=[], total_steps=8, duration_seconds=15.0, failure_reason=None, env_done=True),
]
metrics = compute_metrics(trajs, iteration=0)
assert metrics.total_tasks == 3
assert metrics.successful_tasks == 2
assert abs(metrics.overall_success_rate - 2/3) < 0.01
assert abs(metrics.per_subtask_success_rate['pick'] - 0.5) < 0.01
assert abs(metrics.per_subtask_success_rate['clean'] - 1.0) < 0.01
assert abs(metrics.avg_steps_success - 6.5) < 0.01
assert metrics.avg_steps_failure == 50.0

# Test checkpoint round-trip
with tempfile.TemporaryDirectory() as td:
    cm = CheckpointManager(Path(td))
    cp = IterationCheckpoint(
        iteration=0, total_tasks=3, successful_tasks=2,
        overall_success_rate=0.667, per_subtask_success_rate={'pick': 0.5},
        avg_steps_success=6.5, avg_steps_failure=50.0,
        skill_library_path='skills.json', trajectories_path='traj.jsonl',
        timestamp=1234567890.0
    )
    cm.save(cp)
    loaded = cm.load_latest()
    assert loaded is not None
    assert loaded.iteration == 0
    assert loaded.total_tasks == 3

print('ALL EVAL TESTS PASSED')
"` prints ALL EVAL TESTS PASSED.
  </verify>
  <done>Evaluation orchestrator runs 134 tasks with semaphore-limited concurrency, metrics compute correctly, checkpoints round-trip through atomic writes, agent loop accepts retrieved_skills parameter.</done>
</task>

<task type="auto">
  <name>Task 2: Add evaluate CLI command and verify end-to-end wiring</name>
  <files>
    src/main.py
  </files>
  <action>
    Update `src/main.py` to support subcommands:

    1. **Restructure CLI** with subcommands using argparse subparsers:
       - `python -m src.main run` — Existing single-task execution (move current logic here)
       - `python -m src.main evaluate` — Full 134-task evaluation

    2. **`evaluate` subcommand arguments:**
       - `--iteration` (int, default 0) — Iteration number (0 = baseline with no skills)
       - `--max-concurrent` (int, default 10) — Number of concurrent workers
       - `--max-steps` (int, default 50) — Max steps per task
       - `--output-dir` (str, default "data/experiments") — Base output directory
       - `--skill-library` (str, default "data/skills/skills.json") — Path to skill library JSON
       - `--top-k` (int, default 3) — Number of skills to retrieve per task

    3. **`evaluate` implementation:**
       ```python
       async def evaluate(args):
           # Check API key
           # Create EvaluationOrchestrator
           orchestrator = EvaluationOrchestrator(
               skill_library_path=Path(args.skill_library),
               output_dir=Path(args.output_dir),
               max_concurrent=args.max_concurrent,
               max_steps=args.max_steps,
               top_k_skills=args.top_k,
           )
           # Run iteration
           metrics = await orchestrator.run_iteration(args.iteration)
           # Print final summary
       ```

    4. **Keep backward compatibility** — `python -m src.main` without subcommand still works (default to `run` behavior or print help).

    5. **Do NOT run the actual evaluation** — This task only wires the CLI. The actual iteration 0 run is a manual step (requires DEEPSEEK_API_KEY and ALFWorld environment, takes significant time/money).
  </action>
  <verify>
    Run: `python -m src.main evaluate --help` shows evaluate subcommand with all arguments (--iteration, --max-concurrent, --max-steps, --output-dir, --skill-library, --top-k).
    Run: `python -m src.main run --help` shows run subcommand with existing arguments.
    Run: `python -c "from src.evaluation.orchestrator import EvaluationOrchestrator; print('orchestrator importable')"` succeeds.
  </verify>
  <done>CLI supports `evaluate` subcommand with all required arguments. Full evaluation wiring complete: CLI -> EvaluationOrchestrator -> run_task with skill injection -> metrics -> checkpoint.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.evaluation import EvaluationOrchestrator, compute_metrics, CheckpointManager"` — imports work
2. `python -m src.main evaluate --help` — CLI shows all evaluate options
3. `python -m src.main run --help` — existing run command still works
4. Metrics computation produces correct aggregates for test trajectories
5. Checkpoint round-trips through save/load_latest
6. Agent loop.py accepts retrieved_skills and passes to prompt builder
7. Full wiring: CLI -> Orchestrator -> SkillRetriever -> run_task(retrieved_skills) -> build_prompt_with_skills
</verification>

<success_criteria>
- EvaluationOrchestrator runs 134 tasks with asyncio.Semaphore(10) concurrency
- Each task gets TopK=3 skills retrieved per task description
- Each worker creates its own EnvManager for environment isolation
- compute_metrics returns correct overall/per-subtask/avg-steps metrics
- CheckpointManager saves/loads with atomic writes and latest symlink
- Agent loop accepts retrieved_skills and builds prompt with injected skills
- CLI has evaluate subcommand with iteration, concurrency, output, skill library args
- Per-task errors are caught and logged, not propagated to crash entire evaluation
</success_criteria>

<output>
After completion, create `.planning/phases/02-skill-system-evaluation/02-02-SUMMARY.md`
</output>
