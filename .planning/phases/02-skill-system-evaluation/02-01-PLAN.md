---
phase: 02-skill-system-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/skills/__init__.py
  - src/skills/models.py
  - src/skills/library.py
  - src/skills/retrieval.py
  - src/agent/prompts.py
autonomous: true

must_haves:
  truths:
    - "Skill dataclass stores name, principle, when_to_apply with metadata fields"
    - "SkillLibrary can add, update, remove skills and persists to JSON atomically"
    - "SkillRetriever encodes skills and queries via sentence-transformers, returns TopK via FAISS"
    - "Empty skill library returns empty list from retrieve() without crashing"
    - "Agent prompt includes retrieved skills section when skills are available"
  artifacts:
    - path: "src/skills/models.py"
      provides: "Skill dataclass"
      contains: "class Skill"
    - path: "src/skills/library.py"
      provides: "SkillLibrary with CRUD + atomic JSON persistence"
      contains: "class SkillLibrary"
    - path: "src/skills/retrieval.py"
      provides: "SkillRetriever with sentence-transformers + FAISS"
      contains: "class SkillRetriever"
    - path: "src/agent/prompts.py"
      provides: "Prompt builder with skill injection"
      contains: "def build_prompt_with_skills"
  key_links:
    - from: "src/skills/retrieval.py"
      to: "src/skills/models.py"
      via: "imports Skill dataclass"
      pattern: "from src.skills.models import Skill"
    - from: "src/skills/library.py"
      to: "src/skills/models.py"
      via: "imports Skill dataclass"
      pattern: "from .models import Skill"
    - from: "src/agent/prompts.py"
      to: "src/skills/models.py"
      via: "accepts list[Skill] for prompt injection"
      pattern: "Skill"
---

<objective>
Build the skill library system: Skill data model, library CRUD with atomic JSON persistence, semantic retrieval via sentence-transformers + FAISS, and skill injection into agent prompts.

Purpose: Provides the skill storage and retrieval infrastructure that the evaluation orchestrator (Plan 02) will use to inject relevant skills into agent prompts per-task.

Output: `src/skills/` package with models, library, and retrieval modules; updated `src/agent/prompts.py` with skill injection; updated `pyproject.toml` with new dependencies.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-skill-system-evaluation/02-RESEARCH.md
@src/agent/prompts.py
@src/trajectory/storage.py
@src/trajectory/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create skill data model, library with atomic persistence, and retrieval system</name>
  <files>
    pyproject.toml
    src/skills/__init__.py
    src/skills/models.py
    src/skills/library.py
    src/skills/retrieval.py
  </files>
  <action>
    1. **Update pyproject.toml** — Add `sentence-transformers>=3.4`, `faiss-cpu>=1.13`, `tqdm>=4.66` to dependencies.

    2. **Create `src/skills/__init__.py`** — Export Skill, SkillLibrary, SkillRetriever.

    3. **Create `src/skills/models.py`** — Skill dataclass with fields:
       - `name: str` — Unique skill identifier
       - `principle: str` — The core transferable insight
       - `when_to_apply: str` — Conditions where this skill is relevant
       - `created_iteration: int = 0` — Iteration when skill was created
       - `last_used_iteration: int = 0` — Last iteration skill was retrieved
       - `usage_count: int = 0` — Total times retrieved

    4. **Create `src/skills/library.py`** — SkillLibrary class:
       - `__init__(self, storage_path: Path)` — Sets path, initializes empty dict
       - `load(self)` — Load skills from JSON file, populate `self.skills: dict[str, Skill]` keyed by name
       - `save(self)` — Atomic write: temp file + fsync + os.replace (same pattern as trajectory/storage.py)
       - `add_skill(self, skill: Skill)` — Add/overwrite skill by name, save
       - `update_skill(self, name: str, **kwargs)` — Update specific fields of existing skill, save. Raise KeyError if not found.
       - `remove_skill(self, name: str)` — Delete skill by name, save. Raise KeyError if not found.
       - `get_all_skills(self) -> list[Skill]` — Return list of all skills
       - `__len__(self) -> int` — Number of skills

    5. **Create `src/skills/retrieval.py`** — SkillRetriever class:
       - `__init__(self, model_name: str = "all-MiniLM-L6-v2")` — Load SentenceTransformer model
       - `index_skills(self, skills: list[Skill])` — Encode skill texts as `"{name}: {principle}. {when_to_apply}"`, normalize embeddings, build `faiss.IndexFlatIP` index. Store skills list for result mapping. Handle empty list gracefully (set index to None).
       - `retrieve(self, query: str, top_k: int = 3) -> list[Skill]` — Encode query with `normalize_embeddings=True`, call `faiss.normalize_L2()`, search index, return top_k Skill objects. Return empty list if index is None or skills empty. Cap top_k at len(skills).

    Key implementation details:
    - Use `normalize_embeddings=True` in encode() AND `faiss.normalize_L2()` for cosine similarity via dot product
    - Use `show_progress_bar=False` in encode() calls
    - Use `convert_to_tensor=False` in encode() for numpy arrays compatible with FAISS
    - Follow Phase 1 atomic write pattern exactly (see src/trajectory/storage.py)
    - JSON format: `{"skill_name": {name, principle, when_to_apply, created_iteration, ...}, ...}`
  </action>
  <verify>
    Run: `python -c "from src.skills import Skill, SkillLibrary, SkillRetriever; print('imports ok')"` succeeds.
    Run: `python -c "
from pathlib import Path
from src.skills import Skill, SkillLibrary, SkillRetriever
import tempfile, os

# Test library CRUD
with tempfile.TemporaryDirectory() as td:
    lib = SkillLibrary(Path(td) / 'skills.json')
    lib.load()
    assert len(lib) == 0

    s = Skill(name='test', principle='do X', when_to_apply='when Y')
    lib.add_skill(s)
    assert len(lib) == 1
    assert (Path(td) / 'skills.json').exists()

    lib.remove_skill('test')
    assert len(lib) == 0

# Test retrieval with empty library
r = SkillRetriever()
r.index_skills([])
assert r.retrieve('find a cup') == []

# Test retrieval with skills
skills = [
    Skill(name='search', principle='look everywhere', when_to_apply='finding objects'),
    Skill(name='heat', principle='use microwave', when_to_apply='heating objects'),
]
r.index_skills(skills)
results = r.retrieve('I need to find a mug', top_k=1)
assert len(results) == 1
print('ALL TESTS PASSED')
"` prints ALL TESTS PASSED.
  </verify>
  <done>Skill dataclass defined, SkillLibrary performs CRUD with atomic JSON persistence, SkillRetriever encodes and searches via FAISS with proper normalization, empty library handled gracefully.</done>
</task>

<task type="auto">
  <name>Task 2: Add skill injection to agent prompt builder</name>
  <files>
    src/agent/prompts.py
  </files>
  <action>
    Update `src/agent/prompts.py`:

    1. **Keep existing `AUTONOMOUS_AGENT_PROMPT` constant** unchanged (it serves as the base prompt).

    2. **Add `build_prompt_with_skills(retrieved_skills: list) -> str` function:**
       - Takes a list of Skill objects (import from src.skills.models)
       - If `retrieved_skills` is empty or None, return `AUTONOMOUS_AGENT_PROMPT` unchanged
       - If skills exist, insert a "Relevant Skills" section between the "Available Tools" block and the "Instructions:" block
       - Format each skill as:
         ```
         N. skill_name
            Principle: principle text
            When to apply: when_to_apply text
         ```
       - Add framing: "Consider these skills when planning your approach." after the list
       - Use string `.replace("Instructions:", skills_section + "\nInstructions:")` to insert at the right position

    The function signature should accept `list[Skill]` but also work with `list[dict]` for flexibility (duck-type the field access using getattr with dict fallback).

    Actually, keep it simple: accept `list[Skill]` only. The caller is always the evaluation orchestrator which has Skill objects.

    ```python
    from src.skills.models import Skill

    def build_prompt_with_skills(retrieved_skills: list[Skill] | None = None) -> str:
        if not retrieved_skills:
            return AUTONOMOUS_AGENT_PROMPT

        skills_section = "\n\nRelevant Skills (learned from past experience):\n"
        for i, skill in enumerate(retrieved_skills, 1):
            skills_section += f"\n{i}. {skill.name}\n"
            skills_section += f"   Principle: {skill.principle}\n"
            skills_section += f"   When to apply: {skill.when_to_apply}\n"
        skills_section += "\nConsider these skills when planning your approach.\n"

        return AUTONOMOUS_AGENT_PROMPT.replace(
            "Instructions:",
            skills_section + "\nInstructions:"
        )
    ```
  </action>
  <verify>
    Run: `python -c "
from src.agent.prompts import build_prompt_with_skills, AUTONOMOUS_AGENT_PROMPT
from src.skills.models import Skill

# No skills = base prompt
assert build_prompt_with_skills() == AUTONOMOUS_AGENT_PROMPT
assert build_prompt_with_skills([]) == AUTONOMOUS_AGENT_PROMPT

# With skills = includes skills section
skills = [Skill(name='search', principle='look everywhere', when_to_apply='finding objects')]
prompt = build_prompt_with_skills(skills)
assert 'Relevant Skills' in prompt
assert 'search' in prompt
assert 'look everywhere' in prompt
assert 'Instructions:' in prompt
assert prompt.index('Relevant Skills') < prompt.index('Instructions:')
print('PROMPT TESTS PASSED')
"` prints PROMPT TESTS PASSED.
  </verify>
  <done>build_prompt_with_skills() injects retrieved skills into agent prompt between tools and instructions; returns base prompt when no skills available.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.skills import Skill, SkillLibrary, SkillRetriever"` — imports work
2. `python -c "from src.agent.prompts import build_prompt_with_skills"` — prompt builder importable
3. SkillLibrary round-trips skills through JSON with atomic writes
4. SkillRetriever returns relevant skills for queries and handles empty library
5. build_prompt_with_skills produces correct prompt with and without skills
</verification>

<success_criteria>
- src/skills/ package exists with models.py, library.py, retrieval.py
- Skill dataclass has name, principle, when_to_apply + metadata fields
- SkillLibrary has add/update/remove with atomic JSON persistence
- SkillRetriever uses sentence-transformers + FAISS with proper normalization
- Empty library case handled without crashes
- Agent prompt includes skills section when skills are provided
- pyproject.toml has sentence-transformers, faiss-cpu, tqdm dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/02-skill-system-evaluation/02-01-SUMMARY.md`
</output>
