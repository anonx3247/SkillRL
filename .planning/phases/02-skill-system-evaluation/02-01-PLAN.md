---
phase: 02-skill-system-evaluation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - pyproject.toml
  - src/skills/__init__.py
  - src/skills/models.py
  - src/skills/library.py
  - src/skills/retrieval.py
  - src/skills/server.py
  - src/agent/prompts.py
autonomous: true

must_haves:
  truths:
    - "Skill dataclass stores name, principle, when_to_apply with metadata fields"
    - "SkillLibrary can add, update, remove skills and persists to JSON atomically"
    - "SkillRetriever encodes skills and queries via sentence-transformers, returns TopK via FAISS"
    - "Empty skill library returns empty list from retrieve() without crashing"
    - "Agent prompt includes retrieved skills section when skills are available"
    - "Teacher MCP tools (add_skill, update_skill, remove_skill) wrap SkillLibrary methods for Phase 3 teacher agent"
  artifacts:
    - path: "src/skills/models.py"
      provides: "Skill dataclass"
      contains: "class Skill"
    - path: "src/skills/library.py"
      provides: "SkillLibrary with CRUD + atomic JSON persistence"
      contains: "class SkillLibrary"
    - path: "src/skills/retrieval.py"
      provides: "SkillRetriever with sentence-transformers + FAISS"
      contains: "class SkillRetriever"
    - path: "src/skills/server.py"
      provides: "FastMCP server with add_skill, update_skill, remove_skill tools"
      contains: "class SkillServer"
    - path: "src/agent/prompts.py"
      provides: "Prompt builder with skill injection"
      contains: "def build_prompt_with_skills"
  key_links:
    - from: "src/skills/retrieval.py"
      to: "src/skills/models.py"
      via: "imports Skill dataclass"
      pattern: "from src.skills.models import Skill"
    - from: "src/skills/library.py"
      to: "src/skills/models.py"
      via: "imports Skill dataclass"
      pattern: "from .models import Skill"
    - from: "src/skills/server.py"
      to: "src/skills/library.py"
      via: "MCP tools delegate to SkillLibrary methods"
      pattern: "library\\.add_skill|library\\.update_skill|library\\.remove_skill"
    - from: "src/agent/prompts.py"
      to: "src/skills/models.py"
      via: "accepts list[Skill] for prompt injection"
      pattern: "Skill"
---

<objective>
Build the skill library system: Skill data model, library CRUD with atomic JSON persistence, semantic retrieval via sentence-transformers + FAISS, and skill injection into agent prompts.

Purpose: Provides the skill storage and retrieval infrastructure that the evaluation orchestrator (Plan 02) will use to inject relevant skills into agent prompts per-task.

Output: `src/skills/` package with models, library, retrieval, and MCP server modules; updated `src/agent/prompts.py` with skill injection; updated `pyproject.toml` with new dependencies.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-skill-system-evaluation/02-RESEARCH.md
@src/agent/prompts.py
@src/trajectory/storage.py
@src/trajectory/models.py
@src/environment/server.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create skill data model, library with atomic persistence, and retrieval system</name>
  <files>
    pyproject.toml
    src/skills/__init__.py
    src/skills/models.py
    src/skills/library.py
    src/skills/retrieval.py
  </files>
  <action>
    1. **Update pyproject.toml** — Add `sentence-transformers>=3.4`, `faiss-cpu>=1.13`, `tqdm>=4.66` to dependencies.

    2. **Create `src/skills/__init__.py`** — Export Skill, SkillLibrary, SkillRetriever.

    3. **Create `src/skills/models.py`** — Skill dataclass with fields:
       - `name: str` — Unique skill identifier
       - `principle: str` — The core transferable insight
       - `when_to_apply: str` — Conditions where this skill is relevant
       - `created_iteration: int = 0` — Iteration when skill was created
       - `last_used_iteration: int = 0` — Last iteration skill was retrieved
       - `usage_count: int = 0` — Total times retrieved

    4. **Create `src/skills/library.py`** — SkillLibrary class:
       - `__init__(self, storage_path: Path)` — Sets path, initializes empty dict
       - `load(self)` — Load skills from JSON file, populate `self.skills: dict[str, Skill]` keyed by name
       - `save(self)` — Atomic write: temp file + fsync + os.replace (same pattern as trajectory/storage.py)
       - `add_skill(self, skill: Skill)` — Add/overwrite skill by name, save
       - `update_skill(self, name: str, **kwargs)` — Update specific fields of existing skill, save. Raise KeyError if not found.
       - `remove_skill(self, name: str)` — Delete skill by name, save. Raise KeyError if not found.
       - `get_all_skills(self) -> list[Skill]` — Return list of all skills
       - `__len__(self) -> int` — Number of skills

    5. **Create `src/skills/retrieval.py`** — SkillRetriever class:
       - `__init__(self, model_name: str = "all-MiniLM-L6-v2")` — Load SentenceTransformer model
       - `index_skills(self, skills: list[Skill])` — Encode skill texts as `"{name}: {principle}. {when_to_apply}"`, normalize embeddings, build `faiss.IndexFlatIP` index. Store skills list for result mapping. Handle empty list gracefully (set index to None).
       - `retrieve(self, query: str, top_k: int = 3) -> list[Skill]` — Encode query with `normalize_embeddings=True`, call `faiss.normalize_L2()`, search index, return top_k Skill objects. Return empty list if index is None or skills empty. Cap top_k at len(skills).

    Key implementation details:
    - Use `normalize_embeddings=True` in encode() AND `faiss.normalize_L2()` for cosine similarity via dot product
    - Use `show_progress_bar=False` in encode() calls
    - Use `convert_to_tensor=False` in encode() for numpy arrays compatible with FAISS
    - Follow Phase 1 atomic write pattern exactly (see src/trajectory/storage.py)
    - JSON format: `{"skill_name": {name, principle, when_to_apply, created_iteration, ...}, ...}`
  </action>
  <verify>
    Run: `python -c "from src.skills import Skill, SkillLibrary, SkillRetriever; print('imports ok')"` succeeds.
    Run: `python -c "
from pathlib import Path
from src.skills import Skill, SkillLibrary, SkillRetriever
import tempfile, os

# Test library CRUD
with tempfile.TemporaryDirectory() as td:
    lib = SkillLibrary(Path(td) / 'skills.json')
    lib.load()
    assert len(lib) == 0

    s = Skill(name='test', principle='do X', when_to_apply='when Y')
    lib.add_skill(s)
    assert len(lib) == 1
    assert (Path(td) / 'skills.json').exists()

    lib.remove_skill('test')
    assert len(lib) == 0

# Test retrieval with empty library
r = SkillRetriever()
r.index_skills([])
assert r.retrieve('find a cup') == []

# Test retrieval with skills
skills = [
    Skill(name='search', principle='look everywhere', when_to_apply='finding objects'),
    Skill(name='heat', principle='use microwave', when_to_apply='heating objects'),
]
r.index_skills(skills)
results = r.retrieve('I need to find a mug', top_k=1)
assert len(results) == 1
print('ALL TESTS PASSED')
"` prints ALL TESTS PASSED.
  </verify>
  <done>Skill dataclass defined, SkillLibrary performs CRUD with atomic JSON persistence, SkillRetriever encodes and searches via FAISS with proper normalization, empty library handled gracefully.</done>
</task>

<task type="auto">
  <name>Task 2: Add skill injection to agent prompt builder</name>
  <files>
    src/agent/prompts.py
  </files>
  <action>
    Update `src/agent/prompts.py`:

    1. **Keep existing `AUTONOMOUS_AGENT_PROMPT` constant** unchanged (it serves as the base prompt).

    2. **Add `build_prompt_with_skills(retrieved_skills: list) -> str` function:**
       - Takes a list of Skill objects (import from src.skills.models)
       - If `retrieved_skills` is empty or None, return `AUTONOMOUS_AGENT_PROMPT` unchanged
       - If skills exist, insert a "Relevant Skills" section between the "Available Tools" block and the "Instructions:" block
       - Format each skill as:
         ```
         N. skill_name
            Principle: principle text
            When to apply: when_to_apply text
         ```
       - Add framing: "Consider these skills when planning your approach." after the list
       - Use string `.replace("Instructions:", skills_section + "\nInstructions:")` to insert at the right position

    The function signature should accept `list[Skill]` but also work with `list[dict]` for flexibility (duck-type the field access using getattr with dict fallback).

    Actually, keep it simple: accept `list[Skill]` only. The caller is always the evaluation orchestrator which has Skill objects.

    ```python
    from src.skills.models import Skill

    def build_prompt_with_skills(retrieved_skills: list[Skill] | None = None) -> str:
        if not retrieved_skills:
            return AUTONOMOUS_AGENT_PROMPT

        skills_section = "\n\nRelevant Skills (learned from past experience):\n"
        for i, skill in enumerate(retrieved_skills, 1):
            skills_section += f"\n{i}. {skill.name}\n"
            skills_section += f"   Principle: {skill.principle}\n"
            skills_section += f"   When to apply: {skill.when_to_apply}\n"
        skills_section += "\nConsider these skills when planning your approach.\n"

        return AUTONOMOUS_AGENT_PROMPT.replace(
            "Instructions:",
            skills_section + "\nInstructions:"
        )
    ```
  </action>
  <verify>
    Run: `python -c "
from src.agent.prompts import build_prompt_with_skills, AUTONOMOUS_AGENT_PROMPT
from src.skills.models import Skill

# No skills = base prompt
assert build_prompt_with_skills() == AUTONOMOUS_AGENT_PROMPT
assert build_prompt_with_skills([]) == AUTONOMOUS_AGENT_PROMPT

# With skills = includes skills section
skills = [Skill(name='search', principle='look everywhere', when_to_apply='finding objects')]
prompt = build_prompt_with_skills(skills)
assert 'Relevant Skills' in prompt
assert 'search' in prompt
assert 'look everywhere' in prompt
assert 'Instructions:' in prompt
assert prompt.index('Relevant Skills') < prompt.index('Instructions:')
print('PROMPT TESTS PASSED')
"` prints PROMPT TESTS PASSED.
  </verify>
  <done>build_prompt_with_skills() injects retrieved skills into agent prompt between tools and instructions; returns base prompt when no skills available.</done>
</task>

<task type="auto">
  <name>Task 3: Create skill library MCP server for teacher agent</name>
  <files>
    src/skills/server.py
  </files>
  <action>
    Create `src/skills/server.py` exposing add_skill, update_skill, remove_skill as MCP tools. Follow the same FastMCP pattern as `src/environment/server.py`.

    1. **Create `src/skills/server.py`:**
       ```python
       """FastMCP server for skill library management (used by teacher agent in Phase 3)."""

       from fastmcp import FastMCP
       from pathlib import Path

       from src.skills.models import Skill
       from src.skills.library import SkillLibrary

       mcp = FastMCP(name="SkillLibraryServer")

       # Module-level library instance (caller sets path before use)
       library = SkillLibrary(Path("data/skills/skills.json"))
       ```

    2. **Add `add_skill` tool:**
       ```python
       @mcp.tool()
       def add_skill(name: str, principle: str, when_to_apply: str, iteration: int = 0) -> str:
           """Add a new skill to the library or overwrite an existing one."""
           skill = Skill(name=name, principle=principle, when_to_apply=when_to_apply, created_iteration=iteration)
           library.add_skill(skill)
           return f"Skill '{name}' added successfully."
       ```

    3. **Add `update_skill` tool:**
       ```python
       @mcp.tool()
       def update_skill(name: str, principle: str | None = None, when_to_apply: str | None = None) -> str:
           """Update fields of an existing skill."""
           kwargs = {}
           if principle is not None:
               kwargs["principle"] = principle
           if when_to_apply is not None:
               kwargs["when_to_apply"] = when_to_apply
           library.update_skill(name, **kwargs)
           return f"Skill '{name}' updated successfully."
       ```

    4. **Add `remove_skill` tool:**
       ```python
       @mcp.tool()
       def remove_skill(name: str) -> str:
           """Remove a skill from the library."""
           library.remove_skill(name)
           return f"Skill '{name}' removed successfully."
       ```

    5. **Update `src/skills/__init__.py`** — Also export `server` module's `mcp` if needed, or just keep imports clean. The MCP server will be imported directly by the Phase 3 teacher agent as `from src.skills.server import mcp, library`.

    Key implementation details:
    - Follow same module-level instance pattern as `src/environment/server.py` (module-level `library` variable)
    - Tools return success strings (not dicts/objects) — consistent with ALFWorld server pattern
    - Error handling: let KeyError from update_skill/remove_skill propagate naturally (MCP framework will return error to caller)
  </action>
  <verify>
    Run: `python -c "
from src.skills.server import mcp, library, add_skill, update_skill, remove_skill
import tempfile
from pathlib import Path

# Point library at temp path for testing
td = tempfile.mkdtemp()
library.storage_path = Path(td) / 'skills.json'
library.load()

# Test add
result = add_skill('test_skill', 'always check containers', 'when searching for objects')
assert 'added' in result
assert len(library) == 1

# Test update
result = update_skill('test_skill', principle='check ALL containers')
assert 'updated' in result
assert library.skills['test_skill'].principle == 'check ALL containers'

# Test remove
result = remove_skill('test_skill')
assert 'removed' in result
assert len(library) == 0

print('MCP SERVER TESTS PASSED')
"` prints MCP SERVER TESTS PASSED.
  </verify>
  <done>FastMCP server exposes add_skill, update_skill, remove_skill tools wrapping SkillLibrary methods. Ready for Phase 3 teacher agent to manage skill library via MCP tools.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.skills import Skill, SkillLibrary, SkillRetriever"` — imports work
2. `python -c "from src.agent.prompts import build_prompt_with_skills"` — prompt builder importable
3. SkillLibrary round-trips skills through JSON with atomic writes
4. SkillRetriever returns relevant skills for queries and handles empty library
5. build_prompt_with_skills produces correct prompt with and without skills
6. `python -c "from src.skills.server import mcp, add_skill, update_skill, remove_skill"` — MCP tools importable
</verification>

<success_criteria>
- src/skills/ package exists with models.py, library.py, retrieval.py, server.py
- Skill dataclass has name, principle, when_to_apply + metadata fields
- SkillLibrary has add/update/remove with atomic JSON persistence
- SkillRetriever uses sentence-transformers + FAISS with proper normalization
- Empty library case handled without crashes
- Agent prompt includes skills section when skills are provided
- pyproject.toml has sentence-transformers, faiss-cpu, tqdm dependencies
- FastMCP server exposes add_skill, update_skill, remove_skill tools wrapping SkillLibrary
</success_criteria>

<output>
After completion, create `.planning/phases/02-skill-system-evaluation/02-01-SUMMARY.md`
</output>
