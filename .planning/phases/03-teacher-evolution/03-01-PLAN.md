---
phase: 03-teacher-evolution
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/teacher/__init__.py
  - src/teacher/analyzer.py
  - src/teacher/prompts.py
autonomous: true

must_haves:
  truths:
    - "Teacher analyzes failed trajectories and proposes new skills as abstract transferable principles"
    - "Teacher analyzes successful trajectories and distills strategic patterns"
    - "Skills never mention task-specific objects, locations, or answers"
    - "Teacher proposes add, update, or remove actions with reasons"
  artifacts:
    - path: "src/teacher/__init__.py"
      provides: "Package exports for TeacherAnalyzer, SkillProposal"
    - path: "src/teacher/analyzer.py"
      provides: "TeacherAnalyzer class with batch trajectory analysis"
      exports: ["TeacherAnalyzer", "SkillProposal"]
      min_lines: 80
    - path: "src/teacher/prompts.py"
      provides: "Teacher system prompts with generality constraints"
      min_lines: 30
  key_links:
    - from: "src/teacher/analyzer.py"
      to: "src/agent/client.py"
      via: "DeepSeekClient for LLM calls"
      pattern: "DeepSeekClient"
    - from: "src/teacher/analyzer.py"
      to: "src/trajectory/models.py"
      via: "Trajectory dataclass for input"
      pattern: "Trajectory"
    - from: "src/teacher/analyzer.py"
      to: "src/skills/models.py"
      via: "Skill dataclass for proposals"
      pattern: "Skill"
---

<objective>
Build the teacher analysis system that uses DeepSeek V3.2 to analyze agent trajectories and propose skill library updates.

Purpose: The teacher is the core intelligence of the evolution loop -- it reads trajectories (both successes and failures) and distills abstract, transferable skills. Without the teacher, the skill library cannot evolve.

Output: `src/teacher/` module with TeacherAnalyzer class that takes trajectories and returns SkillProposal objects (add/update/remove actions).
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-teacher-evolution/03-CONTEXT.md
@.planning/phases/03-teacher-evolution/03-RESEARCH.md

@src/agent/client.py
@src/trajectory/models.py
@src/skills/models.py
@src/skills/library.py
@src/skills/retrieval.py
@src/agent/prompts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Teacher prompts with generality enforcement</name>
  <files>src/teacher/__init__.py, src/teacher/prompts.py</files>
  <action>
Create `src/teacher/` package.

Create `src/teacher/prompts.py` with two prompt templates:

1. `FAILURE_ANALYSIS_PROMPT` -- System prompt for analyzing failed trajectories. The prompt must:
   - Instruct the teacher to analyze a batch of failed trajectories and extract lessons
   - STRONGLY enforce skill generality: "NEVER mention specific objects (tomato, mug, apple, knife), specific locations (countertop 1, cabinet 3, fridge 1), specific receptacles, or task-specific details. Skills must be ABSTRACT TRANSFERABLE PRINCIPLES that apply across many different tasks."
   - Include few-shot examples showing GOOD skills ("Always verify you are holding an object before attempting to place it") vs BAD skills ("Check cabinet 3 for the tomato")
   - Request structured JSON output: list of `{"action": "add"|"update"|"remove", "skill_name": str, "principle": str, "when_to_apply": str, "reason": str}`
   - For "update" actions, include "old_skill_name" field to identify which skill to update
   - For "remove" actions, only "skill_name" and "reason" are needed

2. `SUCCESS_ANALYSIS_PROMPT` -- System prompt for analyzing successful trajectories. Same generality constraints but focused on extracting what strategic patterns led to success (efficient search strategies, object manipulation patterns, etc).

Both prompts should include the ALFWorld tool list (copy from AUTONOMOUS_AGENT_PROMPT) so the teacher understands the agent's action space.

Create a helper function `format_trajectory_for_teacher(trajectory: Trajectory) -> str` that compresses a trajectory into a teacher-readable format:
- Task type, success/failure, total steps
- For each step: step number, action name, action args (condensed), observation (truncated to 100 chars)
- Skip the thought field (not useful for teacher analysis, saves tokens)
- Total output should be ~500-1000 tokens per trajectory to fit batches of 10-20 in context

Create `src/teacher/__init__.py` exporting the prompts and formatter.
  </action>
  <verify>
Run `python -c "from src.teacher.prompts import FAILURE_ANALYSIS_PROMPT, SUCCESS_ANALYSIS_PROMPT, format_trajectory_for_teacher; print('Prompts loaded, lengths:', len(FAILURE_ANALYSIS_PROMPT), len(SUCCESS_ANALYSIS_PROMPT))"` -- should print without error and show prompt lengths > 500 chars each.
  </verify>
  <done>Two teacher prompts exist with explicit generality constraints, few-shot examples of good/bad skills, structured JSON output format, and a trajectory compression function.</done>
</task>

<task type="auto">
  <name>Task 2: TeacherAnalyzer with batch trajectory analysis</name>
  <files>src/teacher/analyzer.py, src/teacher/__init__.py</files>
  <action>
Create `src/teacher/analyzer.py` with:

1. `SkillProposal` dataclass:
   - `action`: str ("add", "update", "remove")
   - `skill_name`: str
   - `principle`: str (empty for "remove")
   - `when_to_apply`: str (empty for "remove")
   - `reason`: str
   - `old_skill_name`: str | None (only for "update" actions)

2. `TeacherAnalyzer` class:
   - `__init__(self, client: DeepSeekClient | None = None)` -- uses existing DeepSeekClient (creates one if not passed)
   - `async def analyze_failures(self, trajectories: list[Trajectory], existing_skills: list[Skill], batch_size: int = 10) -> list[SkillProposal]`:
     - Filter to only failed trajectories
     - Process in batches of `batch_size` (default 10)
     - For each batch: format trajectories using `format_trajectory_for_teacher`, include existing skill names as context (so teacher knows what already exists), send to DeepSeek with FAILURE_ANALYSIS_PROMPT as system message
     - Parse JSON response into SkillProposal objects. Use `json.loads` on the response content. If parsing fails (LLM returned non-JSON), try to extract JSON from markdown code blocks (```json...```). If still fails, skip that batch and log a warning.
     - Return combined proposals from all batches
   - `async def analyze_successes(self, trajectories: list[Trajectory], existing_skills: list[Skill], batch_size: int = 10) -> list[SkillProposal]`:
     - Same as above but with successful trajectories and SUCCESS_ANALYSIS_PROMPT
   - `async def analyze_and_propose(self, trajectories: list[Trajectory], existing_skills: list[Skill], batch_size: int = 10) -> list[SkillProposal]`:
     - Convenience method that runs both failure and success analysis
     - Separates trajectories by success/failure
     - Runs both analyses (can be sequential -- no need to overcomplicate)
     - Deduplicates proposals: if two proposals have same skill_name and same action, keep the first one
     - Returns combined deduplicated list
   - `_validate_proposal(self, proposal: SkillProposal) -> bool`:
     - Post-process validation for generality. Reject proposals containing:
       - Numbers following object/location names (regex: `\b(cabinet|drawer|shelf|countertop|fridge|microwave|sinkbasin|desk|bed|sofa|toilet|bathtub|garbagecan)\s+\d+\b`)
       - Common ALFWorld object names used as specifics (regex: `\b(tomato|apple|potato|lettuce|bread|egg|mug|cup|plate|bowl|fork|knife|spoon|pen|pencil|book|cd|cellphone|laptop|pillow|cloth|soapbar|spraybottle|candle|alarmclock|vase|statue|box|keychain|creditcard|remotecontrol|watch|tissuebox|toiletpaper|plunger|scrubbrush|dishsponge|spatula|ladle|butterknife)\s+\d+\b`)
     - Return True if proposal passes, False if rejected (log warning with rejected skill name and reason)

Use temperature=0.7 for teacher calls (matches existing client default). Use the existing DeepSeekClient which already has retry logic and timeout handling.

Update `src/teacher/__init__.py` to export TeacherAnalyzer and SkillProposal.
  </action>
  <verify>
Run `python -c "from src.teacher import TeacherAnalyzer, SkillProposal; from src.teacher.analyzer import TeacherAnalyzer; p = SkillProposal(action='add', skill_name='test', principle='test', when_to_apply='test', reason='test'); print('TeacherAnalyzer imported, SkillProposal works:', p.action)"` -- should print without error.
  </verify>
  <done>TeacherAnalyzer class exists with batch trajectory analysis (failures + successes), JSON parsing with fallback, deduplication, and post-process generality validation rejecting task-specific skills.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.teacher import TeacherAnalyzer, SkillProposal"` -- imports work
2. `python -c "from src.teacher.prompts import FAILURE_ANALYSIS_PROMPT, SUCCESS_ANALYSIS_PROMPT, format_trajectory_for_teacher"` -- prompts importable
3. Teacher prompts contain generality constraints (grep for "NEVER mention specific")
4. SkillProposal has all required fields (action, skill_name, principle, when_to_apply, reason)
5. TeacherAnalyzer has analyze_failures, analyze_successes, analyze_and_propose methods
</verification>

<success_criteria>
- Teacher module exists at src/teacher/ with analyzer.py and prompts.py
- Teacher prompts enforce skill generality with explicit negative examples
- TeacherAnalyzer processes trajectories in batches via DeepSeekClient
- SkillProposal dataclass represents add/update/remove actions
- Post-process validation rejects task-specific skills mentioning ALFWorld objects/locations
- All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-teacher-evolution/03-01-SUMMARY.md`
</output>
