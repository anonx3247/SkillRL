---
phase: 03-teacher-evolution
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/tracking/__init__.py
  - src/tracking/wandb_logger.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "W&B logs success rate, avg steps, and skill count each iteration"
    - "W&B logs per-subtask success rates each iteration"
    - "W&B logs teacher decisions as a table each iteration"
    - "W&B logs skill names as summary list"
    - "Iteration 0 baseline is logged as the first data point"
    - "W&B run finishes cleanly even on exception"
  artifacts:
    - path: "src/tracking/__init__.py"
      provides: "Package exports for ExperimentTracker"
    - path: "src/tracking/wandb_logger.py"
      provides: "ExperimentTracker wrapping wandb for all experiment logging"
      exports: ["ExperimentTracker"]
      min_lines: 60
    - path: "pyproject.toml"
      provides: "wandb dependency added"
      contains: "wandb"
  key_links:
    - from: "src/tracking/wandb_logger.py"
      to: "wandb"
      via: "wandb.init, wandb.log, wandb.Table"
      pattern: "wandb\\."
    - from: "src/tracking/wandb_logger.py"
      to: "src/evaluation/metrics.py"
      via: "AggregateMetrics for metric logging"
      pattern: "AggregateMetrics"
---

<objective>
Build W&B experiment tracking infrastructure that logs all metrics, per-subtask breakdowns, teacher decisions, and skill library state across iterations.

Purpose: W&B provides live experiment visualization. Without it, there's no way to monitor the evolution loop or compare performance across iterations. The user's locked decisions specify exact dashboard layout: multi-panel success rate + avg steps + skill count, teacher decisions table, and skill names list.

Output: `src/tracking/` module with ExperimentTracker class that wraps wandb for all SkillRL-specific logging needs.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-teacher-evolution/03-CONTEXT.md
@.planning/phases/03-teacher-evolution/03-RESEARCH.md

@src/evaluation/metrics.py
@src/skills/models.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add wandb dependency</name>
  <files>pyproject.toml</files>
  <action>
Add `"wandb>=0.24.2"` to the `dependencies` list in `pyproject.toml`.

Then run `uv pip install wandb>=0.24.2` to install it.

NOTE: Use `uv pip install` because system pip is blocked (externally managed environment -- discovered in Phase 2).
  </action>
  <verify>
Run `python -c "import wandb; print('wandb version:', wandb.__version__)"` -- should print version >= 0.24.2.
  </verify>
  <done>wandb is listed in pyproject.toml dependencies and importable.</done>
</task>

<task type="auto">
  <name>Task 2: ExperimentTracker with W&B logging</name>
  <files>src/tracking/__init__.py, src/tracking/wandb_logger.py</files>
  <action>
Create `src/tracking/` package.

Create `src/tracking/wandb_logger.py` with `ExperimentTracker` class:

```python
class ExperimentTracker:
    """W&B experiment tracker for skill evolution."""
```

Methods:

1. `__init__(self, project: str = "skillrl-evolution", config: dict | None = None)`:
   - Store project name and config
   - `self.run = None` (initialized on start)

2. `def start(self, config: dict | None = None)`:
   - Call `wandb.init(project=self.project, config=config or self.config)`
   - Store the run object: `self.run = wandb.run`
   - This is NOT a context manager -- we call start/finish explicitly because the evolution loop controls lifecycle

3. `def finish(self)`:
   - Call `wandb.finish()` if run is active
   - Reset `self.run = None`

4. `def log_iteration(self, iteration: int, metrics: AggregateMetrics, skill_count: int, skill_names: list[str])`:
   - Log core metrics in a single `wandb.log()` call with `step=iteration`:
     - `"iteration"`: iteration number
     - `"success_rate"`: metrics.overall_success_rate
     - `"avg_steps_success"`: metrics.avg_steps_success
     - `"avg_steps_failure"`: metrics.avg_steps_failure
     - `"skill_count"`: skill_count
     - `"successful_tasks"`: metrics.successful_tasks
     - `"total_tasks"`: metrics.total_tasks
   - Log per-subtask success rates as namespaced metrics:
     - For each (task_type, rate) in metrics.per_subtask_success_rate: log `f"subtask/{task_type}"`: rate
     - Use a SEPARATE `wandb.log()` call with same `step=iteration` so they appear in a "subtask" group in the dashboard
   - Log skill names as a W&B Table (one column "skill_name", one row per skill):
     - `table = wandb.Table(columns=["skill_name"], data=[[name] for name in skill_names])`
     - `wandb.log({f"skills/library_iter_{iteration}": table}, step=iteration)`

5. `def log_teacher_decisions(self, iteration: int, proposals: list)`:
   - Accept proposals as list of objects with .action, .skill_name, .reason attributes (SkillProposal duck typing -- don't import from teacher to avoid circular deps)
   - Create W&B Table with columns: ["action", "skill_name", "reason"]
   - Add one row per proposal
   - `wandb.log({f"teacher/decisions_iter_{iteration}": table}, step=iteration)`
   - If proposals is empty, log an empty table (still useful to show no changes happened)

6. `def log_summary(self, best_success_rate: float, best_iteration: int, final_skill_count: int, total_iterations: int)`:
   - Set `wandb.run.summary["best_success_rate"]` = best_success_rate
   - Set `wandb.run.summary["best_iteration"]` = best_iteration
   - Set `wandb.run.summary["final_skill_count"]` = final_skill_count
   - Set `wandb.run.summary["total_iterations"]` = total_iterations

Create `src/tracking/__init__.py` exporting ExperimentTracker.

IMPORTANT: Do NOT use `with wandb.init()` context manager. The evolution loop in Plan 03 will call `tracker.start()` and `tracker.finish()` in a try/finally block to ensure clean shutdown.

IMPORTANT: Use the `step` parameter in `wandb.log()` to control x-axis alignment. This ensures all metrics for the same iteration align on the same x-axis point.
  </action>
  <verify>
Run `python -c "from src.tracking import ExperimentTracker; t = ExperimentTracker(); print('ExperimentTracker imported, project:', t.project)"` -- should print "skillrl-evolution".
  </verify>
  <done>ExperimentTracker exists with log_iteration (core metrics + per-subtask + skill names table), log_teacher_decisions (W&B table), log_summary, start/finish lifecycle methods.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.tracking import ExperimentTracker"` -- imports work
2. `python -c "import wandb; print(wandb.__version__)"` -- wandb installed
3. `grep "wandb" pyproject.toml` -- dependency listed
4. ExperimentTracker has methods: start, finish, log_iteration, log_teacher_decisions, log_summary
5. log_iteration logs success_rate, avg_steps, skill_count, per-subtask rates, and skill names table
</verification>

<success_criteria>
- wandb added to pyproject.toml and installed
- ExperimentTracker class wraps all W&B logging with clean API
- log_iteration handles core metrics, per-subtask breakdown, and skill names
- log_teacher_decisions logs proposals as W&B table
- log_summary sets persistent summary values for run comparison
- No circular imports (tracker does not import teacher module)
</success_criteria>

<output>
After completion, create `.planning/phases/03-teacher-evolution/03-02-SUMMARY.md`
</output>
