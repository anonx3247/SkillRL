---
phase: 03-teacher-evolution
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - src/evolution/__init__.py
  - src/evolution/loop.py
  - src/evolution/convergence.py
  - src/main.py
autonomous: true

must_haves:
  truths:
    - "Evolution loop runs iteration 0 baseline with empty skill library"
    - "After each iteration, teacher analyzes trajectories and proposes skill updates"
    - "Skill library is updated with teacher proposals (add/update/remove)"
    - "Skills with usage_count=0 after 5+ iterations are pruned"
    - "Evolution stops early when success rate plateaus for N consecutive iterations"
    - "All metrics and teacher decisions are logged to W&B each iteration"
    - "CLI command 'evolve' starts the full evolution loop"
  artifacts:
    - path: "src/evolution/__init__.py"
      provides: "Package exports"
    - path: "src/evolution/loop.py"
      provides: "EvolutionLoop orchestrating evaluate-analyze-update cycle"
      exports: ["EvolutionLoop"]
      min_lines: 100
    - path: "src/evolution/convergence.py"
      provides: "ConvergenceDetector with patience-based early stopping"
      exports: ["ConvergenceDetector"]
      min_lines: 20
    - path: "src/main.py"
      provides: "evolve CLI subcommand"
      contains: "evolve"
  key_links:
    - from: "src/evolution/loop.py"
      to: "src/evaluation/orchestrator.py"
      via: "EvaluationOrchestrator.run_iteration()"
      pattern: "orchestrator\\.run_iteration"
    - from: "src/evolution/loop.py"
      to: "src/teacher/analyzer.py"
      via: "TeacherAnalyzer.analyze_and_propose()"
      pattern: "analyzer\\.analyze_and_propose"
    - from: "src/evolution/loop.py"
      to: "src/tracking/wandb_logger.py"
      via: "ExperimentTracker.log_iteration()"
      pattern: "tracker\\.log_iteration"
    - from: "src/evolution/loop.py"
      to: "src/skills/library.py"
      via: "SkillLibrary add/update/remove for applying proposals"
      pattern: "skill_library\\.(add_skill|update_skill|remove_skill)"
    - from: "src/main.py"
      to: "src/evolution/loop.py"
      via: "CLI evolve command"
      pattern: "EvolutionLoop"
---

<objective>
Build the evolution loop that ties everything together: run evaluation, analyze with teacher, update skill library, log to W&B, check convergence, repeat. Add CLI command to start.

Purpose: This is the main experiment loop. It orchestrates the entire evaluate -> analyze -> evolve -> repeat cycle that tests the core hypothesis: can skill evolution alone drive performance improvements on ALFWorld?

Output: `src/evolution/` module with EvolutionLoop class + `evolve` CLI subcommand in main.py.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-teacher-evolution/03-CONTEXT.md
@.planning/phases/03-teacher-evolution/03-RESEARCH.md

# Need summaries from Wave 1 plans
@.planning/phases/03-teacher-evolution/03-01-SUMMARY.md
@.planning/phases/03-teacher-evolution/03-02-SUMMARY.md

@src/evaluation/orchestrator.py
@src/evaluation/metrics.py
@src/evaluation/checkpoint.py
@src/skills/library.py
@src/skills/models.py
@src/skills/retrieval.py
@src/trajectory/models.py
@src/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ConvergenceDetector and EvolutionLoop</name>
  <files>src/evolution/__init__.py, src/evolution/convergence.py, src/evolution/loop.py</files>
  <action>
Create `src/evolution/` package.

**File 1: `src/evolution/convergence.py`**

Create `ConvergenceDetector` class:
- `__init__(self, patience: int = 5, min_delta: float = 0.01)`:
  - `self.patience` = patience (how many iterations without improvement before stopping)
  - `self.min_delta` = min_delta (minimum improvement to count as progress)
  - `self.best_value` = 0.0
  - `self.no_improvement_count` = 0
- `def check(self, current_value: float) -> bool`:
  - If `current_value > self.best_value + self.min_delta`: update best, reset counter, return False
  - Else: increment counter, return True if counter >= patience
- `def reset(self)`: Reset best_value and counter

**File 2: `src/evolution/loop.py`**

Create `EvolutionLoop` class:

`__init__(self, skill_library_path, output_dir, max_iterations=20, patience=5, min_delta=0.01, max_concurrent=10, max_steps=50, top_k_skills=3, prune_after_iterations=5, wandb_project="skillrl-evolution")`:
- Store all config
- Create ConvergenceDetector(patience, min_delta)
- Do NOT create orchestrator, analyzer, or tracker yet (defer to run())

`async def run(self)`:
This is the main evolution loop. Structure:

```
1. Initialize components:
   - skill_library = SkillLibrary(self.skill_library_path)
   - skill_library.load()  # Empty for iteration 0
   - tracker = ExperimentTracker(project=self.wandb_project)
   - tracker.start(config={all hyperparams as dict})
   - analyzer = TeacherAnalyzer()  # Uses default DeepSeekClient
   - orchestrator = EvaluationOrchestrator(skill_library_path, output_dir, max_concurrent, max_steps, top_k_skills)
   - best_success_rate = 0.0
   - best_iteration = 0

2. try/finally with tracker.finish() in finally block

3. For iteration in range(max_iterations):
   a. Print iteration header
   b. Run evaluation: metrics = await orchestrator.run_iteration(iteration)
   c. Log to W&B: tracker.log_iteration(iteration, metrics, len(skill_library), [s.name for s in skill_library.get_all_skills()])
   d. Track best: if metrics.overall_success_rate > best_success_rate: update best
   e. Check convergence (skip for iteration 0): if iteration > 0 and convergence.check(metrics.overall_success_rate): print "Converged", break
   f. If NOT last iteration:
      - Load trajectories for this iteration from orchestrator output path
      - Teacher analysis: proposals = await analyzer.analyze_and_propose(trajectories, skill_library.get_all_skills())
      - Apply proposals to skill library (see apply_proposals below)
      - Log teacher decisions: tracker.log_teacher_decisions(iteration, proposals)
      - Prune unused skills (see prune below)
      - Rebuild FAISS index: the orchestrator will reload library on next run_iteration anyway (it loads from disk each time)
      - Save updated skill library: skill_library.save()

4. After loop: tracker.log_summary(best_success_rate, best_iteration, len(skill_library), iteration+1)
```

`def _apply_proposals(self, skill_library: SkillLibrary, proposals: list[SkillProposal], iteration: int)`:
- For each proposal:
  - If action == "add": create Skill(name=proposal.skill_name, principle=proposal.principle, when_to_apply=proposal.when_to_apply, created_iteration=iteration), call skill_library.add_skill()
  - If action == "update": call skill_library.update_skill(proposal.old_skill_name or proposal.skill_name, principle=proposal.principle, when_to_apply=proposal.when_to_apply). Catch KeyError and log warning if skill not found.
  - If action == "remove": call skill_library.remove_skill(proposal.skill_name). Catch KeyError and log warning if skill not found.
- Print summary: "Applied N proposals: X adds, Y updates, Z removes"

`def _prune_unused_skills(self, skill_library: SkillLibrary, current_iteration: int)`:
- Only prune if current_iteration >= self.prune_after_iterations
- For each skill in skill_library.get_all_skills():
  - If skill.usage_count == 0 and (current_iteration - skill.created_iteration) >= self.prune_after_iterations:
    - Print f"Pruning unused skill: {skill.name} (created iter {skill.created_iteration}, never used in {current_iteration - skill.created_iteration} iterations)"
    - skill_library.remove_skill(skill.name)

`def _load_iteration_trajectories(self, iteration: int) -> list[Trajectory]`:
- Read from `self.output_dir / f"iteration_{iteration:03d}_trajectories.jsonl"`
- Use the trajectory JSONL reading pattern: open file, json.loads each line, Trajectory from dict
- The Trajectory dataclass has a to_dict() method but not from_dict(). Use `Trajectory(**data)` with manual Step reconstruction:
  - For each trajectory dict: reconstruct steps list by creating Step(**step_data) for each step
  - Then create Trajectory with the reconstructed steps
- Return list of trajectories

**IMPORTANT on skill usage tracking:** The current SkillRetriever.retrieve() does NOT update usage_count on skills. The orchestrator retrieves skills but doesn't track which skills were actually used. For pruning to work, we need to update usage_count during evaluation. Add a method to EvolutionLoop:

`def _update_skill_usage(self, skill_library: SkillLibrary, retriever: SkillRetriever, trajectories: list[Trajectory], iteration: int)`:
- This is a simplification: since we can't easily know which specific skills were retrieved for each task from saved trajectories, use a simpler approach:
  - For ALL skills currently in the library, check if they would be retrieved for ANY of the task descriptions in this iteration
  - Actually, simpler: just increment usage_count for all skills that exist during this iteration (they're all candidates for retrieval). This is coarse but avoids needing to re-run retrieval.
  - EVEN SIMPLER and correct: After evaluation completes, we know which tasks ran. Re-run retrieval for each task and count which skills appear. But this is expensive.
  - BEST APPROACH: Modify the evaluation flow slightly. After orchestrator.run_iteration(), the orchestrator already has the retriever built. But we don't have access to which skills were retrieved per task.
  - PRAGMATIC APPROACH: Skip per-task tracking. Instead, just set `last_used_iteration = current_iteration` for ALL skills in the library at each iteration (they're all indexed and available). Then prune based on: if a skill was CREATED more than N iterations ago AND its usage_count is still 0. Since we're setting last_used_iteration for all skills each iteration, instead use a different signal: track which skills the teacher explicitly proposes to REMOVE. The teacher has full context on which skills are useful.
  - FINAL DECISION: Don't do automatic usage-based pruning. Instead, explicitly ask the teacher to propose "remove" actions for unhelpful skills as part of analyze_and_propose(). The teacher already has this capability. The `_prune_unused_skills` method becomes a no-op placeholder that just prints "Pruning delegated to teacher analysis". The Skill model's usage_count field exists but isn't actively used this iteration -- it's there for future refinement.

Actually, let me reconsider. The simplest correct approach:
- After `orchestrator.run_iteration()`, we have access to `skill_library.get_all_skills()` and the retriever is built inside orchestrator. We don't need perfect tracking.
- Just let the TEACHER handle pruning by including existing skills + their metadata in the analysis prompt. The teacher can propose "remove" actions.
- `_prune_unused_skills` can be a simple safety net: remove skills that have existed for N iterations and the teacher has NEVER mentioned updating or keeping them. But without usage data, this is impossible.
- KEEP IT SIMPLE: Remove `_prune_unused_skills`. Pruning is handled entirely by teacher "remove" proposals. This aligns with the research recommendation and avoids complexity.

So the final loop structure is:
1. Evaluate -> 2. Log metrics -> 3. Check convergence -> 4. Teacher analyze -> 5. Apply proposals (including removes) -> 6. Log teacher decisions -> 7. Save library -> 8. Repeat

Create `src/evolution/__init__.py` exporting EvolutionLoop and ConvergenceDetector.
  </action>
  <verify>
Run `python -c "from src.evolution import EvolutionLoop, ConvergenceDetector; c = ConvergenceDetector(patience=3); print('Not converged:', not c.check(0.5)); print('Not converged:', not c.check(0.6)); print('Still not:', not c.check(0.6)); c2 = ConvergenceDetector(patience=2); c2.check(0.5); c2.check(0.5); print('Converged:', c2.check(0.5))"` -- should show convergence detection working correctly.
  </verify>
  <done>EvolutionLoop orchestrates the full evaluate-analyze-evolve cycle. ConvergenceDetector implements patience-based early stopping. Pruning is handled by teacher "remove" proposals.</done>
</task>

<task type="auto">
  <name>Task 2: CLI evolve subcommand</name>
  <files>src/main.py</files>
  <action>
Add `evolve` subcommand to the existing CLI in `src/main.py`.

1. Add import: `from src.evolution.loop import EvolutionLoop`

2. Add `evolve` subparser after the existing `evaluate` subparser:
   ```python
   evolve_parser = subparsers.add_parser(
       "evolve",
       help="Run full skill evolution loop (evaluate -> teach -> update -> repeat)"
   )
   ```
   Arguments:
   - `--max-iterations`, type=int, default=20, help="Maximum evolution iterations (default: 20)"
   - `--patience`, type=int, default=5, help="Iterations without improvement before early stopping (default: 5)"
   - `--min-delta`, type=float, default=0.01, help="Minimum improvement to count as progress (default: 0.01)"
   - `--max-concurrent`, type=int, default=10, help="Maximum concurrent workers (default: 10)"
   - `--max-steps`, type=int, default=50, help="Maximum steps per task (default: 50)"
   - `--output-dir`, type=str, default="data/experiments", help="Output directory (default: data/experiments)"
   - `--skill-library`, type=str, default="data/skills/skills.json", help="Path to skill library JSON (default: data/skills/skills.json)"
   - `--top-k`, type=int, default=3, help="Number of skills to retrieve per task (default: 3)"
   - `--wandb-project`, type=str, default="skillrl-evolution", help="W&B project name (default: skillrl-evolution)"

3. Add `run_evolution_loop` async function:
   ```python
   async def run_evolution_loop(args):
       """Run full skill evolution loop."""
       # Check for API key
       if not os.environ.get("DEEPSEEK_API_KEY"):
           # ... same error message pattern as existing commands
           sys.exit(1)

       # Check for WANDB_API_KEY
       if not os.environ.get("WANDB_API_KEY"):
           print("Warning: WANDB_API_KEY not set. W&B logging may require login.", file=sys.stderr)
           print("Set it via: export WANDB_API_KEY=your-key-here", file=sys.stderr)

       loop = EvolutionLoop(
           skill_library_path=args.skill_library,
           output_dir=args.output_dir,
           max_iterations=args.max_iterations,
           patience=args.patience,
           min_delta=args.min_delta,
           max_concurrent=args.max_concurrent,
           max_steps=args.max_steps,
           top_k_skills=args.top_k,
           wandb_project=args.wandb_project,
       )

       try:
           await loop.run()
       except KeyboardInterrupt:
           print("\n\nInterrupted by user. Exiting...")
           sys.exit(0)
   ```

4. Add dispatch in main(): `elif args.command == "evolve": await run_evolution_loop(args)`

5. Update the no-command help text to include "evolve":
   ```python
   print("No command specified. Use 'run', 'evaluate', or 'evolve'.")
   print("  python -m src.main evolve --max-iterations 20")
   ```
  </action>
  <verify>
Run `python -m src.main evolve --help` -- should show all evolve arguments (max-iterations, patience, min-delta, etc.).
Run `python -m src.main --help` -- should list run, evaluate, and evolve subcommands.
  </verify>
  <done>CLI has `evolve` subcommand with all hyperparameters configurable. Running `python -m src.main evolve` starts the full evolution loop.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.evolution import EvolutionLoop, ConvergenceDetector"` -- imports work
2. `python -m src.main evolve --help` -- shows all arguments
3. `python -m src.main --help` -- lists evolve subcommand
4. ConvergenceDetector correctly detects plateau (unit test in verify step)
5. EvolutionLoop.__init__ accepts all hyperparameters
6. EvolutionLoop.run() method exists and is async
</verification>

<success_criteria>
- EvolutionLoop ties together EvaluationOrchestrator + TeacherAnalyzer + ExperimentTracker + SkillLibrary
- Full cycle: evaluate -> log -> convergence check -> teacher analyze -> apply proposals -> log decisions -> save -> repeat
- Convergence detection stops loop when success rate plateaus
- Teacher proposals applied correctly (add/update/remove)
- W&B tracked from start to finish with clean shutdown in finally block
- CLI `evolve` command exposes all hyperparameters
- Iteration 0 runs with whatever skill library exists (empty for fresh start)
</success_criteria>

<output>
After completion, create `.planning/phases/03-teacher-evolution/03-03-SUMMARY.md`
</output>
