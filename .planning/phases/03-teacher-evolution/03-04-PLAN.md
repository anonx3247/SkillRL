---
phase: 03-teacher-evolution
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - src/skills/retrieval.py
  - src/teacher/analyzer.py
  - src/evolution/loop.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Skill usage_count increments each time a skill is retrieved during evaluation"
    - "Skill last_used_iteration updates to the current iteration on retrieval"
    - "Teacher receives usage statistics (usage_count, last_used_iteration, created_iteration) when evaluating which skills to remove"
    - "Usage data persists to disk after each evaluation iteration"
  artifacts:
    - path: "src/skills/retrieval.py"
      provides: "retrieve() updates usage_count and last_used_iteration on returned Skill objects"
      contains: "usage_count"
    - path: "src/teacher/analyzer.py"
      provides: "Skill context includes usage statistics for informed pruning decisions"
      contains: "usage_count"
    - path: "src/evolution/loop.py"
      provides: "Skill library saved after evaluation to persist usage tracking data"
      contains: "save"
  key_links:
    - from: "src/skills/retrieval.py"
      to: "src/skills/models.py"
      via: "skill.usage_count += 1 in retrieve()"
      pattern: "usage_count.*\\+="
    - from: "src/evolution/loop.py"
      to: "src/skills/library.py"
      via: "skill_library.save() after orchestrator.run_iteration()"
      pattern: "skill_library\\.save"
    - from: "src/teacher/analyzer.py"
      to: "src/skills/models.py"
      via: "usage_count and last_used_iteration in skill context string"
      pattern: "usage_count|last_used_iteration"
---

<objective>
Wire usage tracking so skill pruning decisions are informed by actual retrieval data.

Purpose: Close the one verification gap (TCH-06) — Skill model has usage_count and last_used_iteration fields but they are never updated. The teacher proposes "remove" actions but has no usage data to decide which skills are genuinely unhelpful (never retrieved). This plan wires three things: (1) retriever updates usage fields on retrieved skills, (2) evolution loop persists usage data after evaluation, (3) teacher prompt includes usage stats.

Output: All three files modified, usage tracking fully wired end-to-end.
</objective>

<execution_context>
@/Users/anas/.claude/get-shit-done/workflows/execute-plan.md
@/Users/anas/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-teacher-evolution/03-VERIFICATION.md
@.planning/phases/03-teacher-evolution/03-03-SUMMARY.md

@src/skills/models.py
@src/skills/retrieval.py
@src/skills/library.py
@src/teacher/analyzer.py
@src/teacher/prompts.py
@src/evaluation/orchestrator.py
@src/evolution/loop.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire usage tracking in retriever and persist in evolution loop</name>
  <files>src/skills/retrieval.py, src/evolution/loop.py</files>
  <action>
**src/skills/retrieval.py** — Update `retrieve()` method to track usage on returned skills:

After the FAISS search returns indices and before the return statement (line 97), add an `iteration` parameter to `retrieve()` and update each matched skill:

1. Add `current_iteration: int = 0` parameter to `retrieve(self, query: str, top_k: int = 3, current_iteration: int = 0)`.
2. After building the results list from `indices[0]`, iterate over the result skills and for each:
   - Increment `skill.usage_count += 1`
   - Set `skill.last_used_iteration = current_iteration`
3. Return the results as before.

This is safe for asyncio concurrency because asyncio is single-threaded (no true parallel execution). The Skill objects in `self.indexed_skills` are shared references, so updates are visible to all callers and accumulate correctly across the 134 concurrent tasks within one iteration.

**src/evaluation/orchestrator.py** — Pass iteration to retrieve():

In `_run_tasks_parallel()`, the inner function `run_single_task` already has access to `iteration` parameter. Update line 195:
- Change: `retrieved_skills = retriever.retrieve(task_description, self.top_k_skills)`
- To: `retrieved_skills = retriever.retrieve(task_description, self.top_k_skills, current_iteration=iteration)`

**src/evolution/loop.py** — Persist usage data after evaluation:

In the `run()` method, after `metrics = await orchestrator.run_iteration(iteration)` (line 116), the skill library's in-memory Skill objects have been updated with usage counts by the retriever. But the SkillLibrary was loaded at the top of run() and the retriever was created inside run_iteration() with a separate copy of skills from `skill_library.get_all_skills()`.

The fix: After `orchestrator.run_iteration(iteration)` returns, we need to sync the usage data back. The simplest approach is to have the orchestrator return the retriever's indexed_skills (which have updated counts). But that's invasive.

**Better approach:** Have the EvolutionLoop reload usage data from the orchestrator's retriever. But the retriever is internal to run_iteration().

**Simplest correct approach:** Modify `EvaluationOrchestrator.run_iteration()` to accept an optional `skill_library: SkillLibrary` parameter. When provided, after evaluation completes, update the library's skill objects with usage data from the retriever and save. Actually, this is still too invasive.

**Actual simplest approach:** The SkillRetriever.indexed_skills are copies from `skill_library.get_all_skills()` which returns `list(self.skills.values())`. In Python, `dict.values()` returns views of the SAME Skill objects. So `list(self.skills.values())` creates a new list but the Skill objects inside are the SAME references as in the dict. Therefore, when retrieve() updates `skill.usage_count` on `self.indexed_skills[i]`, it's updating the SAME Skill object that lives in `skill_library.skills[name]`.

Trace the reference chain:
- `skill_library.skills` is `dict[str, Skill]` with Skill object references
- `skill_library.get_all_skills()` returns `list(self.skills.values())` — same object refs
- `retriever.index_skills(all_skills)` stores `self.indexed_skills = skills` — same list, same refs
- `retriever.retrieve()` accesses `self.indexed_skills[i]` — same Skill objects

So the Skill objects are shared. When retrieve() increments usage_count, it's incrementing on the same object that skill_library.skills[name] points to.

BUT: In `orchestrator.run_iteration()` (line 78-84), a NEW SkillLibrary is created and loaded from disk:
```python
skill_library = SkillLibrary(self.skill_library_path)
skill_library.load()
all_skills = skill_library.get_all_skills()
retriever = SkillRetriever()
retriever.index_skills(all_skills)
```
This is a LOCAL skill_library inside run_iteration(), not the same one in EvolutionLoop.run(). So the usage updates will be on the LOCAL library's Skill objects.

**The fix for loop.py:** After `orchestrator.run_iteration(iteration)` completes, reload the skill library from the orchestrator's local save. But the orchestrator doesn't save usage data.

**Correct fix — two changes:**

1. In `EvaluationOrchestrator.run_iteration()`, after all tasks complete and before computing metrics, save the skill library (which now has updated usage counts from retrieval). Add after line 92 (after `_run_tasks_parallel` returns):
```python
# Persist usage tracking data
skill_library.save()
```

2. In `EvolutionLoop.run()`, after `orchestrator.run_iteration(iteration)` returns (line 116), reload the skill library to pick up the usage data that was saved by the orchestrator:
```python
skill_library.load()  # Reload to pick up usage tracking data from evaluation
```

This way:
- Orchestrator creates local SkillLibrary, loads it, creates retriever with same Skill objects
- During 134-task evaluation, retriever.retrieve() updates usage_count/last_used_iteration on those objects
- After evaluation, orchestrator saves the library (with updated usage data) to disk
- EvolutionLoop reloads from disk to sync its in-memory library with the usage data

**Files to actually modify:**

1. `src/skills/retrieval.py`: Add `current_iteration` param to retrieve(), update usage fields
2. `src/evaluation/orchestrator.py`: Pass `iteration` to retrieve(), save skill_library after evaluation
3. `src/evolution/loop.py`: Add `skill_library.load()` after `orchestrator.run_iteration()` to sync usage data

Do NOT modify src/skills/models.py (fields already exist) or src/skills/library.py (save/load already work with all fields via to_dict/from_dict).
  </action>
  <verify>
Run `cd /Users/anas/dev/SkillRL && python -c "
from src.skills.models import Skill
from src.skills.retrieval import SkillRetriever

# Create test skills
skills = [
    Skill(name='test1', principle='principle1', when_to_apply='condition1'),
    Skill(name='test2', principle='principle2', when_to_apply='condition2'),
]

# Index and retrieve
retriever = SkillRetriever()
retriever.index_skills(skills)
results = retriever.retrieve('principle1', top_k=2, current_iteration=5)

# Verify usage tracking
for s in results:
    assert s.usage_count == 1, f'Expected usage_count=1, got {s.usage_count}'
    assert s.last_used_iteration == 5, f'Expected last_used_iteration=5, got {s.last_used_iteration}'
print('Usage tracking works correctly')

# Verify shared references
assert skills[0].usage_count > 0 or skills[1].usage_count > 0, 'Shared references not working'
print('Shared object references confirmed')
"` to verify retriever updates usage fields.
  </verify>
  <done>
SkillRetriever.retrieve() increments usage_count and sets last_used_iteration on each retrieved skill. EvaluationOrchestrator saves updated usage data after evaluation. EvolutionLoop reloads library to sync usage data.
  </done>
</task>

<task type="auto">
  <name>Task 2: Include usage statistics in teacher skill context</name>
  <files>src/teacher/analyzer.py</files>
  <action>
In `src/teacher/analyzer.py`, update both `analyze_failures()` and `analyze_successes()` methods to include usage statistics in the skill context string passed to the teacher LLM.

Currently (lines 89-92 and 168-171), the skill context is built as:
```python
skill_context = "\n".join(
    f"- {skill.name}: {skill.principle}"
    for skill in existing_skills
) if existing_skills else "No skills in library yet."
```

Change both occurrences to include usage stats:
```python
skill_context = "\n".join(
    f"- {skill.name}: {skill.principle} "
    f"[usage: {skill.usage_count} retrievals, "
    f"last used: iter {skill.last_used_iteration}, "
    f"created: iter {skill.created_iteration}]"
    for skill in existing_skills
) if existing_skills else "No skills in library yet."
```

This gives the teacher visibility into:
- **usage_count**: How many times a skill was retrieved across all tasks. Skills with 0 retrievals after several iterations are candidates for removal.
- **last_used_iteration**: When the skill was last relevant. Skills not used recently may be outdated.
- **created_iteration**: When the skill was added. Combined with usage_count, this shows if a skill has had enough opportunity to prove useful.

Also add a brief instruction to the user_message in BOTH analyze_failures and analyze_successes, right before the final line "Propose skill library updates...". Add this line:
```
Note: Usage statistics [usage: N retrievals, last used: iter M, created: iter K] show how often each skill was retrieved. Skills with 0 or very low retrievals after several iterations may not be relevant and could be candidates for removal.
```

Do NOT modify the system prompts in src/teacher/prompts.py. The usage context belongs in the user message (dynamic per-call data), not the system prompt (static instructions).
  </action>
  <verify>
Run `cd /Users/anas/dev/SkillRL && python -c "
import ast, inspect
from src.teacher.analyzer import TeacherAnalyzer

# Read source of analyze_failures
source = inspect.getsource(TeacherAnalyzer.analyze_failures)
assert 'usage_count' in source, 'analyze_failures missing usage_count in skill context'
assert 'last_used_iteration' in source, 'analyze_failures missing last_used_iteration'

# Read source of analyze_successes
source2 = inspect.getsource(TeacherAnalyzer.analyze_successes)
assert 'usage_count' in source2, 'analyze_successes missing usage_count in skill context'
assert 'last_used_iteration' in source2, 'analyze_successes missing last_used_iteration'

print('Teacher analyzer includes usage statistics in both failure and success analysis')
"` to verify usage stats are included in teacher context.
  </verify>
  <done>
Teacher analyzer formats skill context with usage_count, last_used_iteration, and created_iteration for both failure and success analysis methods, giving the teacher LLM data to make informed pruning decisions.
  </done>
</task>

</tasks>

<verification>
1. `SkillRetriever.retrieve()` accepts `current_iteration` parameter and updates `usage_count` and `last_used_iteration` on retrieved skills
2. `EvaluationOrchestrator.run_iteration()` passes `iteration` to `retriever.retrieve()` and saves the skill library after evaluation
3. `EvolutionLoop.run()` reloads skill library after `orchestrator.run_iteration()` to sync usage data
4. Both `analyze_failures()` and `analyze_successes()` include usage statistics in skill context
5. No new files created, no new dependencies, only wiring existing fields to existing code paths
</verification>

<success_criteria>
- TCH-06 gap closed: Usage tracking fields (usage_count, last_used_iteration) are updated during retrieval, persisted after evaluation, and visible to the teacher for informed pruning decisions
- Existing tests still pass (no breaking changes to public interfaces — current_iteration has a default value)
- The full chain works: retrieve() updates fields -> orchestrator saves -> loop reloads -> teacher sees stats -> teacher can propose informed removals
</success_criteria>

<output>
After completion, create `.planning/phases/03-teacher-evolution/03-04-SUMMARY.md`
</output>
