\section{Related Work}
\label{sec:related}

\noindent \textbf{LLM Agents.} The emergence of capable LLMs has catalyzed rapid development in autonomous agent systems~\citep{wei2026agentic}. ReAct \cite{yao2022react} interleaves reasoning and acting, enabling chain-of-thought style planning during interaction, while Reflexion \cite{shinn2023reflexion} introduces verbal reinforcement through self-reflection on past failures. Frameworks like AutoGen \cite{wu2024autogen} and CAMEL \cite{li2023camel} demonstrate general-purpose multi-agent capabilities, featuring automated orchestration and diverse tool integration. While initial efforts focused on constrained tasks like coding or basic arithmetic, these approaches primarily rely on in-context learning (ICL)~\cite{dong2024survey}. However, these agents struggle to scale as tasks become more complex, as they treat every interaction as an isolated event and must start each new task from scratch without any prior knowledge.

\noindent \textbf{Memory Mechanisms in Agents.} To overcome the limitations of finite context windows and the inability of agents to learn from experience, external memory architectures have become a cornerstone of agent design~\cite{hu2025memory,wang2025static}. Early systems primarily utilized a static RAG paradigm or stored raw trajectories as few-shot examples~\cite{wangvoyager,chhikara2025mem0,zhang2025g,wang2024agent}. However, raw trajectories are often token-heavy and contain significant redundancy and noise, which can lead to performance degradation. Current research has moved toward self-improving memory, distilling interactions into higher-level insights or procedural tips~\cite{wang2025mirix,tang2025agent,fang2025memp,zhao2024expel,ouyang2025reasoningbank,wei2025evo}. While some recent work explores updating memory banks via online training to improve efficiency~\cite{zhang2025memevolve,zhang2026memrl}, many existing methods still struggle to distinguish high-value experiences from noise or fail to distill core principles that can guide internal decision-making.

\noindent \textbf{Evolution of Agentic Skills and Reinforcement Learning.} The development of agentic skills~\cite{authropic3}, which are compact, reusable strategies that capture the essence of subtasks, is increasingly viewed through the lens of Continual Learning (CL) and RL. Traditional CL~\cite{parisi2019continual} focuses on knowledge preservation in predefined tasks, but self-evolving agents~\cite{gao2025survey,xia2025agent0,liu2025agent0} aim for active skill acquisition in open-ended environments~\cite{fang2025memp,wang2025mem}. While RL is widely used to align LLMs~\cite{schulman2017proximal,ouyang2022training}, or improve reasoning via rule-based verifiers~\cite{shao2024deepseekmath}, applying it to agentic skills remains challenging due to sparse rewards and long horizons. Unlike previous memory-augmented RL which treats memory as a static or auxiliary source, recent trends suggest that the key to efficient experience transfer lies in abstraction~\cite{wu2025evolver}. Our work builds on this by treating the skill library as a dynamic component that co-evolves with the agent's policy, utilizing RL to refine structured skills through recursive failure analysis.